{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "텍스트 생성 with RNN(text_generation)_한글텍스트.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indiegolab/nlp_class/blob/master/%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%83%9D%EC%84%B1_with_RNN(text_generation)_%ED%95%9C%EA%B8%80%ED%85%8D%EC%8A%A4%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hcD2nPQvPOFM"
      },
      "source": [
        "# 순환 신경망을 활용한 문자열 생성\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/text_generation.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    TensorFlow.org에서 보기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/ko/tutorials/text/text_generation.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    구글 코랩(Colab)에서 실행하기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/ko/tutorials/text/text_generation.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    깃허브(GitHub) 소스 보기</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "이 튜토리얼에서는 문자 기반 순환 신경망(RNN, Recurrent Neural Network)을 사용하여 어떻게 텍스트를 생성하는지 알아보자.\n",
        "\n",
        "\n",
        " Andrej Karpathy의 [순환 신경망의 뛰어난 효율](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)에서 가져온 셰익스피어 데이터셋으로 작업할 것이다.\n",
        "\n",
        "이 데이터 셋에서 문자 시퀀스 (\"Shakespear\")가 주어지면, 시퀀스의 다음 문자(\"e\")를 예측하는 모델을 훈련한다. 모델을 반복하여 호출하면 더 긴 텍스트 시퀀스 생성이 가능하다.  \n",
        ".\n",
        "\n",
        "Note: 런타임 유형: GPU\n",
        "\n",
        "이번 실습은 [tf.keras](https://www.tensorflow.org/programmers_guide/keras)와 [즉시 실행(eager execution)](https://www.tensorflow.org/programmers_guide/eager)을 활용하여 구현된 실행 가능한 코드가 포함한다.\n",
        "\n",
        "다음은 이 튜토리얼의 30번의 에포크(Epoch)로 훈련된 모델에서 \"Q\" 문자열로 시작될 때의 샘플 출력이다.\n",
        "\n",
        "문장 중 일부는 문법적으로 맞지만 대부분 자연스럽지 않다. 이 모델은 단어의 의미를 학습하지는 않았지만, 고려해야 할 점으로 다음과 같은 것들이 있다.\n",
        "\n",
        "* 모델은 문자 기반이다. 훈련이 시작되었을 때, 이 모델은 영어 단어의 철자를 모르거나 심지어 텍스트의 단위가 단어라는 것도 모른다.\n",
        "\n",
        "* 출력의 구조는 대본과 유사하다. 즉, 텍스트 블록은 대개 화자의 이름으로 시작하고 이 이름들은 모든 데이터셋에서 대문자로 씌여져있다.\n",
        "\n",
        "* 아래에 설명된 것처럼 이 모델은 작은 텍스트 배치(각 100자)로 훈련되었으며 논리적인 구조를 가진 더 긴 텍스트 시퀀스를 생성할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### 텐서플로와 다른 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yG_n40gFzf9s",
        "outputId": "83ea22c5-c9ef-4e4c-9c29-cad0939a67cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "#tensorflow 2.0 임포트\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# !pip install -q tensorflow-gpu==2.0.0-rc1\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# !pip show tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjPLb4_kSIXK",
        "colab_type": "code",
        "outputId": "16a3134d-fb5e-4ed3-e953-27e18da10edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AsE2MtlRrul",
        "colab_type": "code",
        "outputId": "f0da83f2-1da6-475e-8d87-9ea9bc7cc727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### 데이터셋 다운로드\n",
        "\n",
        "다음 코드를 실행하여 데이터를 불러온다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### 데이터 읽기\n",
        "\n",
        "먼저, 텍스트를 살펴보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YZ-_Cgs1wjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "kor 한영 데이터가져와서\n",
        "pandas로 가져옴-> list로 변환\n",
        "-> text\n",
        "'''\n",
        "\n",
        "#그외 전처리를 위한 라이브러리\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zgny07U8486",
        "colab_type": "code",
        "outputId": "0f3f30e6-a3ad-4b8a-e260-b8eb1c32e91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#데이터를 사용하기 위해 구글 드라이브를 마운트 한다. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH4Ogmjg87fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 파일 경로를 가져온다\n",
        "path_to_file = \"/content/drive/My Drive/kor.xlsx\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m4hsOzg88-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#전처리 함수\n",
        "def preprocess_sentence(w):\n",
        "    w = w.lower().strip()\n",
        "\n",
        "    # 문장의 마침표를 공백으로 띄어주는 과정\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    # 영어, 마침표, 물음표, 느낌표, 쉼표 이외의 문자나 특수문자는 전부 제외한다 <- 한글 데이터 사용 시 코드 수정\n",
        "    # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    #좌 우 공백을 제거한다\n",
        "    w = w.rstrip().strip()\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw1sxw6d-kqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. 강조표현을 제거한다\n",
        "# 2. 문장을 깔끔하게 다듬는다.\n",
        "def create_dataset(path, num_examples):\n",
        "    df=pd.read_excel(path, sheet_name='Sheet1')\n",
        "    df.ko = df.ko.apply(preprocess_sentence)\n",
        "    \n",
        "    # 글자로 나누기\n",
        "    return list(df.ko)[:num_examples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoxPqG8tNTur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def listToString(s):  \n",
        "    \n",
        "    # initialize an empty string \n",
        "    str1 = \"\"  \n",
        "    \n",
        "    # traverse in the string   \n",
        "    for ele in s:  \n",
        "        str1 += ele   \n",
        "    \n",
        "    # return string   \n",
        "    return str1  \n",
        "                \n",
        "# Driver code     \n",
        "# print(listToString(text)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55G1WD38AnQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = create_dataset(path_to_file, 10000)\n",
        "text = listToString(text)\n",
        "# text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO1ZH_83NUeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cke7jx2nY6Uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text = \"안녕하세요. 딥러닝 활용 자연어처리 책입니다. 만나서 반갑습니다.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zFhBhvO-knJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#가장 긴 문장의 길이를 가져온다\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBFkxL6zBh0z",
        "colab_type": "code",
        "outputId": "61b3e2a2-e837-454b-99fb-61bc14567ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "max_length(text)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obsukw8A_U8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dfToList = df['ko'][:1000].tolist()\n",
        "# # 1000개 문장을 list로\n",
        "# print(\"dfToList is\", dfToList, \"and it's a\", type(dfToList))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aavnuByVymwK",
        "outputId": "d8534731-d7c8-4428-f1bf-740e6462c006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 텍스트의 길이는 그 안에 있는 문자의 수입니다.\n",
        "print ('텍스트의 길이: {}자'.format(len(text)))"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "텍스트의 길이: 245212자\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Duhg9NrUymwO",
        "outputId": "f4d66ba7-360c-4e92-f190-4c200c1dd716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 텍스트의 처음 250자를 살펴봅니다\n",
        "print(text[:250])"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "나는 매일 저녁 배트를 만나러 다락방으로 가요 .선생님 이문장이 이해가 안 가요 .컴퓨터를 시작하면 시간이 너무 빠르게 가요 .나는 오늘 자정에 한국으로 돌아 가요 .나는 일어나자마자 화장실에 가요 .지금 잠을 자면 깨어나지 못할 거 같아서 지금 가요 .학교가 끝나자마자 기숙사로 가요 .대한민국 남자라면 모두 20 대에 의무적으로 군대에 가요 .오늘밤에 비자 때문에 한국에 가요 .오늘은 새 자동차를 받으러 가요 .급한 일이 있어서 손님 만나러 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlCgQBRVymwR",
        "outputId": "0899a347-dbda-449e-d9ee-50ed30298fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# 파일의 고유 문자수를 출력합니다.\n",
        "vocab = sorted(set(text))\n",
        "print ('고유 문자수 {}개'.format(len(vocab)))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "고유 문자수 1185개\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## 텍스트 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### 텍스트 벡터화\n",
        "\n",
        "훈련 전, 문자들을 수치화할 필요가 있다. 두 개의 조회 테이블(lookup table)을 만든다:  \n",
        "하나는 문자를 숫자에 매핑하고 다른 하나는 숫자를 문자에 매핑하는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IalZLbvOzf-F",
        "colab": {}
      },
      "source": [
        "# 고유 문자에서 인덱스로 매핑 생성\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "이제 각 문자에 대한 정수 표현을 만들었습니다. 문자를 0번 인덱스부터 고유 문자 길이까지 매핑한 것을 기억합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FYyNlCNXymwY",
        "outputId": "3742cc2c-b455-4ec5-e573-8bc3ef916851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  ' ' :   0,\n",
            "  '!' :   1,\n",
            "  '%' :   2,\n",
            "  '&' :   3,\n",
            "  ',' :   4,\n",
            "  '.' :   5,\n",
            "  '0' :   6,\n",
            "  '1' :   7,\n",
            "  '2' :   8,\n",
            "  '3' :   9,\n",
            "  '4' :  10,\n",
            "  '5' :  11,\n",
            "  '6' :  12,\n",
            "  '7' :  13,\n",
            "  '8' :  14,\n",
            "  '9' :  15,\n",
            "  '?' :  16,\n",
            "  '\\\\':  17,\n",
            "  '\\xa0':  18,\n",
            "  'ᄏ' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l1VKcQHcymwb",
        "outputId": "209fb0ce-eb38-4f3a-fbe9-43ee060594ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 텍스트에서 처음 13개의 문자가 숫자로 어떻게 매핑되었는지를 보여줍니다\n",
        "print ('{} ---- 문자들이 다음의 정수로 매핑되었습니다 ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'나는 매일 저녁 배트를 ' ---- 문자들이 다음의 정수로 매핑되었습니다 ---- > [ 165  227    0  439  798    0  822  200    0  503 1048  415    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### 예측 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "주어진 문자나 문자 시퀀스가 주어졌을 때, 다음 문자로 가장 가능성 있는 문자는 무엇일까?\n",
        "\n",
        "\n",
        " 이는 모델을 훈련하여 수행할 작업입니다. 모델의 입력은 문자열 시퀀스가 될 것이고, 모델을 훈련시켜 출력을 예측합니다. 이 출력은 현재 타임 스텝(time step)의 다음 문자입니다.\n",
        "\n",
        "RNN은 이전에 본 요소에 의존하는 내부 상태를 유지하고 있으므로, 이 순간까지 계산된 모든 문자를 감안할 때, 다음 문자는 무엇일까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### 훈련 샘플과 타깃 만들기\n",
        "\n",
        "다음으로 텍스트를 샘플 시퀀스로 나눕니다. 각 입력 시퀀스에는 텍스트에서 나온 `seq_length`개의 문자가 포함될 것입니다.\n",
        "\n",
        "각 입력 시퀀스에서, 해당 타깃은 한 문자를 오른쪽으로 이동한 것을 제외하고는 동일한 길이의 텍스트를 포함합니다.\n",
        "\n",
        "따라서 텍스트를`seq_length + 1`개의 청크(chunk)로 나눕니다. 예를 들어, `seq_length`는 4이고 텍스트를 \"Hello\"이라고 가정해 봅시다. 입력 시퀀스는 \"Hell\"이고 타깃 시퀀스는 \"ello\"가 됩니다.\n",
        "\n",
        "이렇게 하기 위해 먼저 `tf.data.Dataset.from_tensor_slices` 함수를 사용해 텍스트 벡터를 문자 인덱스의 스트림으로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0UHJDA39zf-O",
        "outputId": "932b0841-20be-465b-9c88-062fc6c31b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "# 단일 입력에 대해 원하는 문장의 최대 길이\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# 훈련 샘플/타깃 만들기\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "나\n",
            "는\n",
            " \n",
            "매\n",
            "일\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "`batch` 메서드는 이 개별 문자들을 원하는 크기의 시퀀스로 쉽게 변환할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4hkDU3i7ozi",
        "outputId": "18cccd33-bfc2-4fe3-f7dd-4f7c2a2215df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))\n",
        "\n",
        "# idx2cahr를 통해 index를 문자로 다시 변환했을 때 제대로 출력되는 것을 확인할 수 있다."
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'나는 매일 저녁 배트를 만나러 다락방으로 가요 .선생님 이문장이 이해가 안 가요 .컴퓨터를 시작하면 시간이 너무 빠르게 가요 .나는 오늘 자정에 한국으로 돌아 가요 .나는 일어나자마'\n",
            "'자 화장실에 가요 .지금 잠을 자면 깨어나지 못할 거 같아서 지금 가요 .학교가 끝나자마자 기숙사로 가요 .대한민국 남자라면 모두 20 대에 의무적으로 군대에 가요 .오늘밤에 비자 '\n",
            "'때문에 한국에 가요 .오늘은 새 자동차를 받으러 가요 .급한 일이 있어서 손님 만나러 가요 .그는 조금 있으면 수원으로 가요 .동물병원도 있기 때문에 강아지들을 데리고도 자주 가요 '\n",
            "'.씻고 교복을 입고 학교로 가요 .내일 아침 일찍 얼음 낚시 가요 .다른 어떤 일을 하기 위해 서울 가요 .영화 같은 일이 현실이 되어 가요 .나도 항상 일찍 일어나서 학교를 가요 '\n",
            "'.나는 이제 일 마치고 집에 가요 .그녀는 목요일에 항상 피아노 학원에 가요 .그녀는 화요일에 항상 할아버지 집에 가요 .우리는 금요일마다 음악 학원에 가요 .우리는 일주일에 5일 '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "각 시퀀스에서, `map` 메서드를 사용해 각 배치에 간단한 함수를 적용하고 입력 텍스트와 타깃 텍스트를 복사 및 이동합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NGu-FkO_kYU",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hiCopyGZymwi"
      },
      "source": [
        "첫 번째 샘플의 타깃 값을 출력합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GNbw-iR0ymwj",
        "outputId": "12bd7106-7a01-436f-a933-7df1c5dfc7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('타깃 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력 데이터:  '나는 매일 저녁 배트를 만나러 다락방으로 가요 .선생님 이문장이 이해가 안 가요 .컴퓨터를 시작하면 시간이 너무 빠르게 가요 .나는 오늘 자정에 한국으로 돌아 가요 .나는 일어나자'\n",
            "타깃 데이터:  '는 매일 저녁 배트를 만나러 다락방으로 가요 .선생님 이문장이 이해가 안 가요 .컴퓨터를 시작하면 시간이 너무 빠르게 가요 .나는 오늘 자정에 한국으로 돌아 가요 .나는 일어나자마'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_33OHL3b84i0"
      },
      "source": [
        "이 벡터의 각 인덱스는 하나의 타임 스텝(time step)으로 처리됩니다. 타임 스텝 0의 입력으로 모델은 \"F\"의 인덱스를 받고 다음 문자로 \"i\"의 인덱스를 예측합니다. 다음 타임 스텝에서도 같은 일을 하지만 RNN은 현재 입력 문자 외에 이전 타임 스텝의 컨텍스트**(context)**를 고려합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0eBu9WZG84i0",
        "outputId": "d1554fb7-c30e-4db0-de4b-fc5c8a0ca28f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "# 'First' 각 문자에 대해 입력과 출력을 확인\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"{:4d}단계\".format(i))\n",
        "    print(\"  입력: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  예상 출력: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0단계\n",
            "  입력: 165 ('나')\n",
            "  예상 출력: 227 ('는')\n",
            "   1단계\n",
            "  입력: 227 ('는')\n",
            "  예상 출력: 0 (' ')\n",
            "   2단계\n",
            "  입력: 0 (' ')\n",
            "  예상 출력: 439 ('매')\n",
            "   3단계\n",
            "  입력: 439 ('매')\n",
            "  예상 출력: 798 ('일')\n",
            "   4단계\n",
            "  입력: 798 ('일')\n",
            "  예상 출력: 0 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### 훈련 배치 생성\n",
        "\n",
        "텍스트를 다루기 쉬운 시퀀스로 분리하기 위해 `tf.data`를 사용했습니다. 그러나 이 데이터를 모델에 넣기 전에 데이터를 섞은 후 배치를 만들어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p2pGotuNzf-S",
        "outputId": "a5bc42c9-9255-4af8-82f9-a5b67cc2668a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# 배치 크기\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# 데이터셋을 섞을 버퍼 크기\n",
        "# (TF 데이터는 무한한 시퀀스와 함께 작동이 가능하도록 설계되었으며,\n",
        "# 따라서 전체 시퀀스를 메모리에 섞지 않습니다. 대신에,\n",
        "# 요소를 섞는 버퍼를 유지합니다).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "모델을 정의하려면 `tf.keras.Sequential`을 사용합니다. 이 간단한 예제에서는 3개의 층을 사용하여 모델을 정의합니다:\n",
        "\n",
        "* `tf.keras.layers.Embedding` : 입력층. `embedding_dim` 차원 벡터에 각 문자의 정수 코드를 매핑하는 훈련 가능한 검색 테이블.\n",
        "* `tf.keras.layers.GRU` : 크기가 `units = rnn_units`인 RNN의 유형(여기서 LSTM층을 사용할 수도 있습니다.)\n",
        "* `tf.keras.layers.Dense` : 크기가 `vocab_size`인 출력을 생성하는 출력층."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zHT8cLh7EAsg",
        "colab": {}
      },
      "source": [
        "# 문자로 된 어휘 사전의 크기\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 임베딩 차원\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN 유닛(unit) 개수\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MtCrdfzEI2N0",
        "colab": {}
      },
      "source": [
        "# 모델 정의\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wwsrpOik5zhv",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "각 문자에 대해 모델은 임베딩을 검색하고, 임베딩을 입력으로 하여 GRU를 1개의 타임 스텝으로 실행하고, 완전연결층을 적용하여 다음 문자의 로그 가능도(log-likelihood)를 예측하는 로짓을 생성합니다:\n",
        "\n",
        "![모델을 통과하는 데이터의 사진](https://tensorflow.org/tutorials/text/images/text_generation_training.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## 모델 사용\n",
        "\n",
        "이제 모델을 실행하여 원하는대로 동작하는지 확인합니다.\n",
        "\n",
        "먼저 출력의 형태를 확인합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-_70kKAPrPU",
        "outputId": "8ae2b167-063c-43cf-df63-44311d72246c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 1185) # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "위 예제에서 입력의 시퀀스 길이는 100이지만 모델은 임의 길이의 입력에서 실행될 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vPGmAAXmVLGC",
        "outputId": "7acdf570-f4e4-4edb-e3fb-974d4a199b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (64, None, 256)           303360    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (64, None, 1185)          1214625   \n",
            "=================================================================\n",
            "Total params: 6,764,961\n",
            "Trainable params: 6,764,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "모델로부터 실제 예측을 얻으려면 출력 배열에서 샘플링하여 실제 문자 인덱스를 얻어야 합니다. 이 분포는 문자 어휘에 대한 로짓에 의해 정의됩니다.\n",
        "\n",
        "Note: 배열에 argmax를 취하면 모델이 쉽게 루프에 걸릴 수 있으므로 배열에서 샘플링하는 것이 중요합니다.\n",
        "\n",
        "배치의 첫 번째 샘플링을 시도해 봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4V4MfFg0RQJg",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "이렇게 하면 각 타임 스텝(time step)에서 다음 문자 인덱스에 대한 예측을 제공합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YqFMUQc_UFgM",
        "outputId": "dd7b8e22-4a5f-4332-be1e-bd211e67ef1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 779,  183, 1040,   61,  624,  352,  784, 1023, 1070,   13, 1029,\n",
              "        815, 1020,  145,   75, 1080,  828,  406, 1015,  274,  745,  183,\n",
              "       1129,   82,  170,  969,  911,  371,  553,  411,  124, 1176,  916,\n",
              "        837,  670, 1011,  835,  629,  346,  975,  565,  238,  662,  340,\n",
              "        699, 1161, 1104,  501,  173, 1075,  355,  752,  666,  800,  815,\n",
              "       1027,  736, 1020, 1047,  127,  441,  816,  489,  460,  589,  991,\n",
              "        747,  321,  119,  122,  383,  321,  205,  686,  725,  945,  624,\n",
              "        206, 1122,  731,  871,  989,   53,   53,  323,  280,  229,  262,\n",
              "        528,  109,  139,  279,   72,  953,  514,  996,  773,  187,  593,\n",
              "        162])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "훈련되지 않은 모델에 의해 예측된 텍스트를 보기 위해 복호화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWcFwPwLSo05",
        "outputId": "094edfab-9faa-4189-a9ce-02cbe9207538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "print(\"입력: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"예측된 다음 문자: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력: \n",
            " '어 .당신은 사람을 기쁘게 하는 재능을 가지고 있어 .그곳은 음악을 들을 수도 있고 춤을 출수도 있어 .예술은 마음을 치유해 주는 힘이 있어 .러셀은 자신을 포함해서 다른 물건 들'\n",
            "\n",
            "예측된 다음 문자: \n",
            " '웬냇통겹쇼랑유터팽7테잤탠꾹공펭접룻탑돈옛냇허괴남캣찻럿빔률깨흡책져썰탁젤술락컷뻬님싼뜩액횡픔방났페래옵쌓잃잤텀연탠튜꺼맨장및모샐쿤옥땐깎깜렬땐노아엉충쇼녹핸엘진쿄겐겐떠동늙덜복긍꼽돗곰치범퀵웅냥샴낀'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## 모델 훈련"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "이 문제는 표준 분류 문제로 취급될 수 있습니다. 이전 RNN 상태와 이번 타임 스텝(time step)의 입력으로 다음 문자의 클래스를 예측합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### 옵티마이저 붙이기, 그리고 손실 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "표준 `tf.keras.losses.sparse_softmax_crossentropy` 손실 함수는 이전 차원의 예측과 교차 적용되기 때문에 이 문제에 적합합니다.\n",
        "\n",
        "이 모델은 로짓을 반환하기 때문에 `from_logits` 플래그를 설정해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4HrXTACTdzY-",
        "outputId": "f7bfe8db-302f-4759-9f4b-b9100f53e3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# 손실 함수 정의\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"예측 배열 크기(shape): \", example_batch_predictions.shape, \" # (배치 크기, 시퀀스 길이, 어휘 사전 크기\")\n",
        "print(\"스칼라 손실:          \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "예측 배열 크기(shape):  (64, 100, 1185)  # (배치 크기, 시퀀스 길이, 어휘 사전 크기\n",
            "스칼라 손실:           7.077508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jeOXriLcymww"
      },
      "source": [
        "`tf.keras.Model.compile` 메서드를 사용하여 훈련 절차를 설정합니다. 기본 매개변수의 `tf.keras.optimizers.Adam`과 손실 함수를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDl1_Een6rL0",
        "colab": {}
      },
      "source": [
        "# adam 손실 함수 사용\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### 체크포인트 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C6XBUUavgF56"
      },
      "source": [
        "`tf.keras.callbacks.ModelCheckpoint`를 사용하여 훈련 중 체크포인트(checkpoint)가 저장되도록 합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6fWTriUZP-n",
        "colab": {}
      },
      "source": [
        "# 체크포인트가 저장될 디렉토리\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# 체크포인트 파일 이름\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### 훈련 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "훈련 시간이 너무 길지 않도록 모델을 훈련하는 데 10개의 에포크(Epoch)를 사용합니다. 코랩(Colab)에서는 런타임을 GPU로 설정하여 보다 빠르게 훈련할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7yGBE2zxMMHs",
        "colab": {}
      },
      "source": [
        "EPOCHS=30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UK-hmKjYVoll",
        "outputId": "17c78fd2-7ac4-4d3d-f2a8-f0d0bb6749fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "# 점점 Loss가 줄어듦을 알 수 있다."
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
            "37/37 [==============================] - 10s 271ms/step - loss: 4.6957\n",
            "Epoch 2/30\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 4.0497\n",
            "Epoch 3/30\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 3.7132\n",
            "Epoch 4/30\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 3.5011\n",
            "Epoch 5/30\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 3.3312\n",
            "Epoch 6/30\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 3.1578\n",
            "Epoch 7/30\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 3.0064\n",
            "Epoch 8/30\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 2.8973\n",
            "Epoch 9/30\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 2.8088\n",
            "Epoch 10/30\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 2.7282\n",
            "Epoch 11/30\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 2.6592\n",
            "Epoch 12/30\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 2.5990\n",
            "Epoch 13/30\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 2.5457\n",
            "Epoch 14/30\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 2.4964\n",
            "Epoch 15/30\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 2.4531\n",
            "Epoch 16/30\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 2.4150\n",
            "Epoch 17/30\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 2.3812\n",
            "Epoch 18/30\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 2.3429\n",
            "Epoch 19/30\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 2.3141\n",
            "Epoch 20/30\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 2.2853\n",
            "Epoch 21/30\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 2.2560\n",
            "Epoch 22/30\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 2.2286\n",
            "Epoch 23/30\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 2.2024\n",
            "Epoch 24/30\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 2.1816\n",
            "Epoch 25/30\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 2.1566\n",
            "Epoch 26/30\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 2.1306\n",
            "Epoch 27/30\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 2.1101\n",
            "Epoch 28/30\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 2.0882\n",
            "Epoch 29/30\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 2.0635\n",
            "Epoch 30/30\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 2.0409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## 텍스트 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### 최근 체크포인트 복원"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LyeYRiuVjodY"
      },
      "source": [
        "이 예측 단계를 간단히 유지하기 위해 배치 크기로 1을 사용합니다.\n",
        "\n",
        "RNN 상태가 타임 스텝에서 타임 스텝으로 전달되는 방식이기 때문에 모델은 한 번 빌드된 고정 배치 크기만 허용합니다.\n",
        "\n",
        "다른 배치 크기로 모델을 실행하려면 모델을 다시 빌드하고 체크포인트에서 가중치를 복원해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zk2WJ2-XjkGz",
        "outputId": "e047b960-9772-4082-8821-44297f55dddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LycQ-ot_jjyu",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71xa6jnYVrAN",
        "outputId": "995c1b6c-fca9-45e2-ee91-001dd329e141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (1, None, 256)            303360    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (1, None, 1185)           1214625   \n",
            "=================================================================\n",
            "Total params: 6,764,961\n",
            "Trainable params: 6,764,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### 예측 루프\n",
        "\n",
        "다음 코드 블록은 텍스트를 생성합니다:\n",
        "\n",
        "* 시작 문자열 선택과 순환 신경망 상태를 초기화하고 생성할 문자 수를 설정하면 시작됩니다.\n",
        "\n",
        "* 시작 문자열과 순환 신경망 상태를 사용하여 다음 문자의 예측 배열을 가져옵니다.\n",
        "\n",
        "* 다음, 범주형 배열을 사용하여 예측된 문자의 인덱스를 계산합니다. 이 예측된 문자를 모델의 다음 입력으로 활용합니다.\n",
        "\n",
        "* 모델에 의해 리턴된 RNN 상태는 모델로 피드백되어 이제는 하나의 단어가 아닌 더 많은 컨텍스트를 갖추게 됩니다. 다음 단어를 예측한 후 수정된 RNN 상태가 다시 모델로 피드백되어 이전에 예측된 단어에서 더 많은 컨텍스트를 얻으면서 학습하는 방식입니다.\n",
        "\n",
        "![텍스트를 생성하기 위해 모델의 출력이 입력으로 피드백](https://tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
        "\n",
        "생성된 텍스트를 보면 모델이 언제 대문자로 나타나고, 절을 만들고 셰익스피어와 유사한 어휘를 가져오는지 알 수 있습니다. 훈련 에포크(Epoch)가 적은 관계로 논리적인 문장을 형성하는 것은 아직 훈련되지 않았습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n",
        "\n",
        "  # 생성할 문자의 수\n",
        "  num_generate = 1000\n",
        "\n",
        "  # 시작 문자열을 숫자로 변환(벡터화)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # 결과를 저장할 빈 문자열\n",
        "  text_generated = []\n",
        "\n",
        "  # 온도가 낮으면 더 예측 가능한 텍스트가 됩니다.\n",
        "  # 온도가 높으면 더 의외의 텍스트가 됩니다.\n",
        "  # 최적의 세팅을 찾기 위한 실험\n",
        "  temperature = 1.0\n",
        "\n",
        "  # 여기에서 배치 크기 == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # 배치 차원 제거\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # 예측된 단어를 다음 입력으로 모델에 전달\n",
        "      # 이전 은닉 상태와 함께\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktovv0RFhrkn",
        "outputId": "ca226dcc-c93d-4db3-9eb8-7aef88c88cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"정말 \"))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정말 폭르나오는 종류의 이 버지를 확실하고 싶어 .당신이 프로드에서 양혹을 모망할 수 있지 .초등이 큰 감이가 비프라 , 폰의 상관에 충수할 수 있어 .축구의 불량이 아니가 내고 생활하고 싶어 .나는 주사 오색 이메일을 받으니라는 할으로 돈으로 나의 이겨 나올지 않아 .우 차리상사할 때 포츠의 활익들도 예측할 수능없어 .내지 확실히 하면 내티시를 볼 수 있어 .가방은 아두에서 구집을 내용기 위해 자주 들이고 있어 .좋은 저즈국은 집중점에 갔어 진 천체 있고 건강한 것이 그 노력에 있어 .금정 들이 이뤄지 않으면 끝나길 바래 .이제부터는 아이것이 마디어트가 송절적인 게 없어 .그는 페이지에는 크게 마별을 모끼겠던 것 다니 내 출풍이 그룹긴 변내가에 갔어 .우리는 공전권 우리의 관정샘플을 통해야하고 있어 .너의 돈을 위해서 시간 같이 늘이나서 따라울인 것도록 여행을 갔어 .나의 산기객 순산은 아껴지고 있어 .육관한 사람은 얌전에 달려가 없어 .미국이 메일을 작곡을 가족하면 될까 ?나는 이상할증이 뉴는데 아람 식사망을 주고 있어 .이번주 운전체 예약을 참고했다면 당신의 계획을 위한 연락을 판매할 것 같아 .생각의 정음으로 그걸 색깔을 우리면 눌러 갔어 .우린 바라는 노래를 부분 할 수 있지 않아 .바닥 도착해주면 건거 할 수 없어 .메일로 숙소한 시험에 갈깝지 않아 .아리디린 순간은 결혼한 터 그들을 위해 준비해보는 노치를 많아 .우리는 전달해서 얼마나 안 있는 것 같아 .그렇게 존재 더워르면 난 그 그것을 금융 하게에서 일을 해주길 바래 .나는 능력 여성의 사무실을 훔래 .나는 너가 이 옷임에 걸릴것 같아 .사랑 함께 후 아디에서는 검거색 카드를 즐길 수 없어 .내 오래 .나는 부멍된 영어를 두 개의 마실장 회신과 절대등 가지 친구를 써 알고 있어 .그들은 남편과체에 진식과 시대가 있어 .그 룸였을 때 잔울을 부분없게 놀 거 같아 .할색다운 미위로 큰 별씨가 되고 있어 .완벽히라는 데리남에서 일하여 사무실을 내용할 수 없어 .지금도 그곳에 간절적으로 우리 무엇을 수량대\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "결과를 개선하는 가장 쉬운 방법은 더 오래 훈련하는 것입니다(`EPOCHS = 30`을 시도해 봅시다).\n",
        "\n",
        "또한 다른 시작 문자열을 시험해 보거나 모델의 정확도를 높이기 위해 다른 RNN 레이어를 추가하거나 온도 파라미터를 조정하여 많은 혹은 적은 임의의 예측을 생성할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## 고급: 맞춤식 훈련\n",
        "\n",
        "위의 훈련 절차는 간단하지만 많은 권한을 부여하지는 않습니다.\n",
        "\n",
        "이제 수동으로 모델을 실행하는 방법을 살펴 보았으니 이제 훈련 루프를 해제하고 직접 구현합시다.\n",
        "이는 시작점을 제공해 주는데, 예를 들어 커리큘럼 학습(curriculum learning)을 구현하면 모델의 오픈 루프(open-loop) 출력을 안정적으로 하는 데 도움을 줍니다.\n",
        "\n",
        "기울기 추적을 위해 `tf.GradientTape`을 사용합니다. 이 방법에 대한 자세한 내용은 [즉시 실행 가이드](https://www.tensorflow.org/guide/eager)를 참조하십시오.\n",
        "\n",
        "절차는 다음과 같이 동작합니다:\n",
        "\n",
        "* 먼저 RNN 상태를 초기화합니다. 우리는`tf.keras.Model.reset_states` 메서드를 호출하여 이를 수행합니다.\n",
        "\n",
        "* 다음으로 데이터셋(배치별로)를 반복하고 각각에 연관된 *예측*을 계산합니다.\n",
        "\n",
        "* `tf.GradientTape`를 열고 그 컨텍스트에서의 예측과 손실을 계산합니다.\n",
        "\n",
        "* `tf.GradientTape.grads` 메서드를 사용하여 모델 변수에 대한 손실의 기울기를 계산합니다.\n",
        "\n",
        "* 마지막으로 옵티마이저의 `tf.train.Optimizer.apply_gradients` 메서드를 사용하여 이전 단계로 이동합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_XAm7eCoKULT",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qUKhnZtMVpoJ",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4kH1o0leVIp",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d4tSNwymzf-q",
        "colab": {}
      },
      "source": [
        "# 훈련 횟수\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # 모든 에포크(Epoch)의 시작에서 은닉 상태를 초기화\n",
        "  # 초기 은닉 상태는 None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = '에포크 {} 배치 {} 손실 {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # 모든 5 에포크(Epoch)마다(체크포인트) 모델 저장\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('에포크 {} 손실 {:.4f}'.format(epoch+1, loss))\n",
        "  print ('1 에포크 당 {}초 소요\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}